import os
import sys
sys.path.append(os.path.abspath("./src"))
import joblib
import pandas as pd
import shap

from src.features.feature_engineering import feature_engineering_function


BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_PATH = os.path.join(BASE_DIR, "..", "models", "final_pipeline_LightGBM.pkl")

pipeline = joblib.load(MODEL_PATH)
model = pipeline.named_steps["model"]
explainer = shap.TreeExplainer(model)


def get_selected_feature_names():
    preprocess = pipeline.named_steps["preprocess"]
    try:
        names = preprocess.get_feature_names_out()
    except:
        names = []
        for _, _, cols in preprocess.transformers_:
            names.extend(cols)

    selector = pipeline.named_steps["feature_selection"]
    return [names[i] for i in selector.get_support(indices=True)]


def explain_prediction(input_json):

    df_raw = pd.DataFrame([input_json])

    # Apply Feature Engineering
    X_fe = feature_engineering_function(df_raw)

    # Preprocessing transform
    X_pp = pipeline.named_steps["preprocess"].transform(X_fe)

    # Feature selection
    X = pipeline.named_steps["feature_selection"].transform(X_pp)

    # Model prediction
    prob = pipeline.predict_proba(df_raw)[0][1]

    # SHAP Calculation
    shap_vals = explainer.shap_values(X)
    shap_values = shap_vals[1] if isinstance(shap_vals, list) else shap_vals

    features = get_selected_feature_names()
    df_shap = pd.DataFrame(
        {
            "feature": features,
            "shap_value": shap_values[0],
        }
    ).sort_values(by="shap_value", ascending=False)

    for i, row in df_shap.iterrows():
        feature_name = row["feature"]
        if feature_name in X_fe.columns:
            df_shap.at[i, "feature_value"] = X_fe[feature_name].iloc[0]
        else:
            df_shap.at[i, "feature_value"] = "â€”"

    explanation = create_human_explanation(df_shap, prob)

    return df_shap, prob, explanation


def create_human_explanation(shap_df, prob=None):

    increasing = shap_df[shap_df["shap_value"] > 0].head(5)
    decreasing = shap_df[shap_df["shap_value"] < 0].tail(5)

    feature_translations = {
        "tot_cur_bal_log": "Ù…Ø§Ù†Ø¯Ù‡ Ú©Ù„ Ø­Ø³Ø§Ø¨â€ŒÙ‡Ø§",
        "dti": "Ù†Ø³Ø¨Øª Ø¨Ø¯Ù‡ÛŒ Ø¨Ù‡ Ø¯Ø±Ø¢Ù…Ø¯",
        "home_ownership_enc": "Ù…Ø§Ù„Ú©ÛŒØª Ø®Ø§Ù†Ù‡",
        "installment_to_fico": "Ù†Ø³Ø¨Øª Ù‚Ø³Ø· Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø§Ø¹ØªØ¨Ø§Ø±ÛŒ",
        "term_int_rate": "Ù†Ø±Ø® Ø¨Ù‡Ø±Ù‡ Ùˆ Ù…Ø¯Øª ÙˆØ§Ù…",
        "grade_rank": "Ø±ØªØ¨Ù‡ Ø§Ø¹ØªØ¨Ø§Ø±ÛŒ",
        "fico_dti_interaction": "Ù†Ø³Ø¨Øª Ø¨Ø¯Ù‡ÛŒ Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø§Ø¹ØªØ¨Ø§Ø±ÛŒ",
        "installment_to_income": "Ù†Ø³Ø¨Øª Ù‚Ø³Ø· Ø¨Ù‡ Ø¯Ø±Ø¢Ù…Ø¯",
        "purpose_term_interaction": "Ù†ÙˆØ¹ ÙˆØ§Ù… Ùˆ Ù…Ø¯Øª Ø¢Ù†",
        "open_acc_group_num": "ØªØ¹Ø¯Ø§Ø¯ Ø­Ø³Ø§Ø¨â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²",
        "purpose_risk": "Ø±ÛŒØ³Ú© Ù†ÙˆØ¹ ÙˆØ§Ù…",
        "tot_cur_bal_group_num": "Ú¯Ø±ÙˆÙ‡ ØªØ±Ø§Ø² Ø­Ø³Ø§Ø¨â€ŒÙ‡Ø§",
        "fico_income_interaction": "ØªØ±Ú©ÛŒØ¨ Ø¯Ø±Ø¢Ù…Ø¯ Ùˆ Ø§Ù…ØªÛŒØ§Ø² Ø§Ø¹ØªØ¨Ø§Ø±ÛŒ",
        "fico_bin_num": "Ø¨Ø§Ø²Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø§Ø¹ØªØ¨Ø§Ø±ÛŒ",
        "revol_util_to_fico": "Ù†Ø³Ø¨Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ø¹ØªØ¨Ø§Ø± Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø§Ø¹ØªØ¨Ø§Ø±ÛŒ",
        "income_group": "Ú¯Ø±ÙˆÙ‡ Ø¯Ø±Ø¢Ù…Ø¯ÛŒ",
        "fico_balance_interaction": "Ù†Ø³Ø¨Øª Ù…Ø§Ù†Ø¯Ù‡ Ø­Ø³Ø§Ø¨ Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø§Ø¹ØªØ¨Ø§Ø±ÛŒ",
        "installment_to_income_interaction": "ØªØ¹Ø§Ù…Ù„ Ù†Ø³Ø¨Øª Ù‚Ø³Ø· Ùˆ Ø¯Ø±Ø¢Ù…Ø¯",
    }

    if prob is None:
        prob = 0
    if prob < 0.4:
        risk_label = "ðŸŸ¢ Ù…Ø´ØªØ±ÛŒ ØªÙˆØ§Ù† Ø¨Ø§Ø²Ù¾Ø±Ø¯Ø§Ø®Øª ÙˆØ§Ù… Ø±Ø§ Ø¯Ø§Ø±Ø¯ Ùˆ Ø±ÛŒØ³Ú© Ù†Ú©ÙˆÙ„ Ù¾Ø§ÛŒÛŒÙ† Ø§Ø³Øª."
    elif prob < 0.65:
        risk_label = "ðŸŸ¡ Ù…Ø´ØªØ±ÛŒ Ø±ÛŒØ³Ú© Ù…ØªÙˆØ³Ø·ÛŒ Ø¯Ø± Ø¨Ø§Ø²Ù¾Ø±Ø¯Ø§Ø®Øª ÙˆØ§Ù… Ø¯Ø§Ø±Ø¯."
    else:
        risk_label = "ðŸ”´ Ø§ÛŒÙ† ÙˆØ§Ù… Ø¨Ø±Ø§ÛŒ Ù…Ø´ØªØ±ÛŒ Ø±ÛŒØ³Ú© Ø¨Ø§Ù„Ø§ÛŒÛŒ Ø§Ø² Ù†Ø¸Ø± Ø¹Ø¯Ù… Ø¨Ø§Ø²Ù¾Ø±Ø¯Ø§Ø®Øª Ø¯Ø§Ø±Ø¯."

    explanation = f"ðŸ“Š Ø§Ø­ØªÙ…Ø§Ù„ Ù†Ú©ÙˆÙ„: {prob * 100:.1f}%\n{risk_label}\n"
    explanation += " Ù…Ù† Ø´Ø±Ø§ÛŒØ· Ø±Ùˆ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø±Ø¯Ù… Ùˆ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³ÙˆØ§Ø¨Ù‚ Ù…Ø´ØªØ±ÛŒ Ù‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒØŒ Ù†ØªØ§ÛŒØ¬ Ø²ÛŒØ± Ø¨Ù‡ Ø¯Ø³Øª Ø§ÙˆÙ…Ø¯Ù‡ ðŸ‘‡\n\n"

    explanation += "ðŸš¨ Ø¹ÙˆØ§Ù…Ù„ÛŒ Ú©Ù‡ Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ù†Ù‚Ø´ Ø±Ø§ Ø¯Ø± Ø§ÙØ²Ø§ÛŒØ´ Ø±ÛŒØ³Ú© Ù†Ú©ÙˆÙ„ Ø¯Ø§Ø´ØªÙ†Ø¯:\n"
    for _, row in increasing.iterrows():
        f = row["feature"]
        fa_name = feature_translations.get(f, f.replace("_", " "))
        explanation += f"â€¢ {fa_name} ({f}) \n"

    if len(decreasing) > 0:
        explanation += "\n Ø¹ÙˆØ§Ù…Ù„ÛŒ  Ú©Ù‡ Ø¨Ø§Ø¹Ø« Ú©Ø§Ù‡Ø´ Ø±ÛŒØ³Ú© Ù†Ú©ÙˆÙ„ Ù…ÛŒØ´Ù†:\n"
        for _, row in decreasing.iterrows():
            f = row["feature"]
            fa_name = feature_translations.get(f, f.replace("_", " "))
            explanation += f"â€¢ {fa_name} ({f}) \n"

    high_risk_feats = [
        feature_translations.get(f, f.replace("_", " "))
        for f in increasing["feature"].tolist()[:3]
    ]
    low_risk_feats = [
        feature_translations.get(f, f.replace("_", " "))
        for f in decreasing["feature"].tolist()[:3]
    ]
    if prob > 0.65:

        explanation += "\nðŸ’¡ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ø±ÛŒØ³Ú©ØŒ Ø¨Ù‡ØªØ± Ø§Ø³Øª Ø±ÙˆÛŒ Ù…ÙˆØ§Ø±Ø¯ Ø²ÛŒØ± ØªÙ…Ø±Ú©Ø² Ú©Ù†ÛŒØ¯:\n"
        explanation += "Ú©Ø§Ù‡Ø´ " + ", ".join(high_risk_feats)
    return explanation
